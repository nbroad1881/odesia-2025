{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Exist 2022 - Task 1](#exist-2022---task-1)  \n",
    "[Exist 2022 - Task 2](#exist-2022---task-2)  \n",
    "[Exist 2023 - Task 1](#exist-2023---task-1)  \n",
    "[Exist 2023 - Task 2](#exist-2023---task-2)  \n",
    "[Exist 2023 - Task 3](#exist-2023---task-3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loaded .env:\", load_dotenv(\"../../.env\", override=True))\n",
    "data_dir = Path(os.environ[\"PROJECT_DIR\"]) / \"data\" / \"host\"\n",
    "\n",
    "LANG = \"es\"\n",
    "SPLIT = \"val\"\n",
    "\n",
    "from prompts import (\n",
    "    prompt_t1_exist_2022,\n",
    "    prompt_t2_exist_2022,\n",
    "    prompt_t1_exist_2023,\n",
    "    prompt_t2_exist_2023,\n",
    "    prompt_t3_exist_2023,\n",
    "    numbered_t2_exist_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exist 2022 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an excellent assistant at identifying sexism in spanish tweets. You must decide whether or not a given tweet expresses ideas related to sexism in any of the three forms: it is sexist itself, it describes a sexist situation in which discrimination towards women occurs, or criticizes a sexist behaviour. The following statements show examples of sexist and not sexist messages, respectively.  Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\n",
      "\n",
      "\n",
      "Sexist tweet: Woman driving, be careful!\n",
      "Not sexist tweet: Just saw a woman wearing a mask outside spank her very tightly leashed dog and I gotta say I love learning absolutely everything about a stranger in a single instant.\n",
      "\n",
      "After thoroughly reading and analyzing the tweet, respond with either \"sexist\" or \"not sexist\".\n",
      "\n",
      "\n",
      "Tweet:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_t1_exist_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e771aade67dc4794826ca1b68874c092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1_2022 = json.load(open(data_dir / \"exist_2022\" / f\"{SPLIT}_t1_{LANG}.json\"))\n",
    "\n",
    "print(len(t1_2022))\n",
    "\n",
    "label2str = {\"sexist\": \"sexist\", \"non-sexist\": \"not sexist\"}\n",
    "\n",
    "\n",
    "def format_t1_2022(x):\n",
    "    return {\n",
    "        \"text\": prompt_t1_exist_2022 + x[\"text\"],\n",
    "        \"response\": label2str[x[\"value\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "ds_t1_2022 = Dataset.from_list(t1_2022)\n",
    "\n",
    "ds_t1_2022 = ds_t1_2022.map(format_t1_2022, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_case': 'EXIST2021',\n",
       " 'id': '11199',\n",
       " 'source': 'twitter',\n",
       " 'language': 'es',\n",
       " 'text': 'You are an excellent assistant at identifying sexism in spanish tweets. You must decide whether or not a given tweet expresses ideas related to sexism in any of the three forms: it is sexist itself, it describes a sexist situation in which discrimination towards women occurs, or criticizes a sexist behaviour. The following statements show examples of sexist and not sexist messages, respectively.  Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\\n\\n\\nSexist tweet: Woman driving, be careful!\\nNot sexist tweet: Just saw a woman wearing a mask outside spank her very tightly leashed dog and I gotta say I love learning absolutely everything about a stranger in a single instant.\\n\\nAfter thoroughly reading and analyzing the tweet, respond with either \"sexist\" or \"not sexist\".\\n\\n\\nTweet:\\n\\nMarico estaba embarazada, tuve cesárea y cuidaba y bañaba al bebé. Que loco',\n",
       " 'value': 'non-sexist',\n",
       " 'response': 'not sexist'}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_t1_2022.shuffle()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exist 2022 - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an excellent assistant at categorizing sexism in spanish tweets. Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\n",
      "\n",
      "Here are the category definitions:\n",
      "\n",
      "1. Ideological and inequality. This category includes all tweets that discredit the feminist movement in order to devalue, belittle and defame the struggle of women in any aspect\n",
      "of their lives. It also includes messages that reject inequality between men and women, or present men as victims of gender-based oppression. Some examples of this type of sexism discourse can be found in the following tweets:  \"#Feminism is a war on men, but it's also a war on women. It's a war on female nature, a war on femininity.\"\n",
      "2. Role stereotyping and dominance. This category includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks. It also includes any claim that implies that men are somehow superior to women. \"Most women no longer have the desire or the knowledge to develop a high quality character, even if they wanted to.\"\n",
      "3. Objectification. Objectification and physical stereotyping includes messages where women are presented as objects apart from their dignity and personal aspects. We also include messages that assume or describe certain physical qualities that women must have in order to fulfill traditional gender roles, for example, ideas that suggest that women should maintain a standard and ideal of beauty or attacks on a woman's physique. \"No offense but I've never seen an attractive african american hooker. Not a single one\"\n",
      "4. Sexual violence. This category includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made: \"I wanna touch your tits..you can't imagine what I can do on your body\"\n",
      "5. Misogyny and non sexual violence: this category includes expressions of hatred and violence towards women: \"Domestic abuse is never okay.... Unless your wife is a bitch.\"\n",
      "6. Not sexist. No sexism present in the text.\n",
      "\n",
      "For this task, you will be shown a tweet in Spanish. After thoroughly reading and analyzing the tweet, categorize the tweet into ONLY ONE of the six categories by using the following labels:\n",
      "\n",
      "1. ideological inequality\n",
      "2. stereotyping dominance\n",
      "3. sexual violence\n",
      "4. misogyny and/or non-sexual violence\n",
      "5. objectification\n",
      "6. not sexist\n",
      "\n",
      "\n",
      "Tweet:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_t2_exist_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e890d4a27b524a29bb7adf894e5d3cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_2022 = json.load(open(data_dir / \"exist_2022\" / f\"{SPLIT}_t2_{LANG}.json\"))\n",
    "\n",
    "print(len(t2_2022))\n",
    "\n",
    "label2str = {\n",
    "    \"non-sexist\": numbered_t2_exist_labels[5],\n",
    "    \"ideological-inequality\": numbered_t2_exist_labels[0],\n",
    "    \"stereotyping-dominance\": numbered_t2_exist_labels[1],\n",
    "    \"sexual-violence\": numbered_t2_exist_labels[2],\n",
    "    \"misogyny-non-sexual-violence\": numbered_t2_exist_labels[3],\n",
    "    \"objectification\": numbered_t2_exist_labels[4],\n",
    "}\n",
    "\n",
    "\n",
    "def format_t2_2022(x):\n",
    "    return {\n",
    "        \"text\": prompt_t2_exist_2022 + x[\"text\"],\n",
    "        \"response\": label2str[x[\"value\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "ds_t2_2022 = Dataset.from_list(t2_2022)\n",
    "ds_t2_2022 = ds_t2_2022.map(format_t2_2022, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_case': 'EXIST2021',\n",
       " 'id': '11219',\n",
       " 'source': 'twitter',\n",
       " 'language': 'es',\n",
       " 'text': 'You are an excellent assistant at categorizing sexism in spanish tweets. Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\\n\\nHere are the category definitions:\\n\\n1. Ideological and inequality. This category includes all tweets that discredit the feminist movement in order to devalue, belittle and defame the struggle of women in any aspect\\nof their lives. It also includes messages that reject inequality between men and women, or present men as victims of gender-based oppression. Some examples of this type of sexism discourse can be found in the following tweets:  \"#Feminism is a war on men, but it\\'s also a war on women. It\\'s a war on female nature, a war on femininity.\"\\n2. Role stereotyping and dominance. This category includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks. It also includes any claim that implies that men are somehow superior to women. \"Most women no longer have the desire or the knowledge to develop a high quality character, even if they wanted to.\"\\n3. Objectification. Objectification and physical stereotyping includes messages where women are presented as objects apart from their dignity and personal aspects. We also include messages that assume or describe certain physical qualities that women must have in order to fulfill traditional gender roles, for example, ideas that suggest that women should maintain a standard and ideal of beauty or attacks on a woman\\'s physique. \"No offense but I\\'ve never seen an attractive african american hooker. Not a single one\"\\n4. Sexual violence. This category includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made: \"I wanna touch your tits..you can\\'t imagine what I can do on your body\"\\n5. Misogyny and non sexual violence: this category includes expressions of hatred and violence towards women: \"Domestic abuse is never okay.... Unless your wife is a bitch.\"\\n6. Not sexist. No sexism present in the text.\\n\\nFor this task, you will be shown a tweet in Spanish. After thoroughly reading and analyzing the tweet, categorize the tweet into ONLY ONE of the six categories by using the following labels:\\n\\n1. ideological inequality\\n2. stereotyping dominance\\n3. sexual violence\\n4. misogyny and/or non-sexual violence\\n5. objectification\\n6. not sexist\\n\\n\\nTweet:\\n\\n@amabilicia @mariafdezv @EstefaniaVeloz Y ahora la feminazi en su manual: pasó #2 descalificar... Es fácil de comprender su manual, pues no procede la la razón, sino de la dictatoriedad... La corteza cerebral y el embrión? Jajajajajajajaja deja de fumar la porquería, es un humilde consejo. https://t.co/um9VMGV4aC',\n",
       " 'value': 'ideological-inequality',\n",
       " 'response': '1. ideological inequality'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_t2_2022.shuffle()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exist 2023 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an excellent assistant at identifying sexism in spanish tweets. You must decide whether or not a given tweet expresses ideas related to sexism in any of the three forms: it is sexist itself, it describes a sexist situation in which discrimination towards women occurs, or criticizes a sexist behaviour. The following statements show examples of sexist and not sexist messages, respectively.  Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\n",
      "\n",
      "\n",
      "Sexist tweet: Woman driving, be careful!\n",
      "Not sexist tweet: Just saw a woman wearing a mask outside spank her very tightly leashed dog and I gotta say I love learning absolutely everything about a stranger in a single instant.\n",
      "\n",
      "After thoroughly reading and analyzing the tweet, respond with either \"sexist\" or \"not sexist\".\n",
      "\n",
      "\n",
      "Tweet:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_t1_exist_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e791329087ee49038c111ff0167ac696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693d27914f7c437e8de2eb182ed7590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1_2023 = json.load(open(data_dir / \"exist_2023\" / f\"{SPLIT}_t1_{LANG}.json\"))\n",
    "\n",
    "print(len(t1_2023))\n",
    "\n",
    "def format_t1_2023(x):\n",
    "\n",
    "    yes_count = x[\"value\"].count(\"YES\")\n",
    "\n",
    "    return {\n",
    "        \"text\": prompt_t1_exist_2023 + x[\"tweet\"],\n",
    "        \"response\": \"sexist\" if yes_count > 3 else \"not sexist\",\n",
    "    }\n",
    "\n",
    "def filter_t1_2023(x):\n",
    "    \"\"\"\n",
    "    Filter out samples that don't have agreement of at least 2/3 of the annotators.\n",
    "    \"\"\"\n",
    "    labels = list(chain(*x[\"value\"]))\n",
    "\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    for k, v in counts.items():\n",
    "        if v >=4:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "ds_t1_2023 = Dataset.from_list(t1_2023)\n",
    "ds_t1_2023 = ds_t1_2023.filter(filter_t1_2023).map(format_t1_2023, remove_columns=ds_t1_2023.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'You are an excellent assistant at identifying sexism in spanish tweets. You must decide whether or not a given tweet expresses ideas related to sexism in any of the three forms: it is sexist itself, it describes a sexist situation in which discrimination towards women occurs, or criticizes a sexist behaviour. The following statements show examples of sexist and not sexist messages, respectively.  Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\\n\\n\\nSexist tweet: Woman driving, be careful!\\nNot sexist tweet: Just saw a woman wearing a mask outside spank her very tightly leashed dog and I gotta say I love learning absolutely everything about a stranger in a single instant.\\n\\nAfter thoroughly reading and analyzing the tweet, respond with either \"sexist\" or \"not sexist\".\\n\\n\\nTweet:\\n\\nEsta publicidad solo incluye a gais y bisexuales hombres cisgénero. El androcentrismo se expresa hasta en las luchas más bonitas. Qué mal. https://t.co/7ImlUzsBTB',\n",
       " 'response': 'sexist'}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_t1_2023.shuffle()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exist 2023 - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"You are an excellent assistant at identifying the source in sexist spanish tweets. Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex. \n",
      "\n",
      "Category definitions:\n",
      "\n",
      "1. Direct sexist message. The intention was to write a message that is sexist by itself or incites to be sexist, as in: \"Women shouldn't code...perhaps be influencer/creator instead...it's their natural\n",
      "strength.\"\n",
      "2. Reported sexist message. The intention is to report and share a sexist situation suffered by a woman or women in first or third person, as in: \"Today, one of my year 1 class pupils could not believe he'd lost a race against a girl.\"\n",
      "3. Judgemental message. The intention was judgmental, since the tweet describes sexist situations or behaviours with the aim of condemning them. As in: \"21st century and we are still earning 25% less than men #Idonotrenounce.\"\n",
      "4. Not sexist message. No sexism present in the text.\n",
      "\n",
      "For this task, you will be shown a tweet in Spanish. After thoroughly reading and analyzing the tweet, categorize the tweet into one of the above categories using the following labels:\n",
      "\n",
      "1. direct\n",
      "2. reported\n",
      "3. judgmental\n",
      "4. not sexist\n",
      "\n",
      "\n",
      "Tweet:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_t2_exist_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b985d2d096447bb00f5ce254e5b02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61df2f45e8e4d598219afcb4b4d963f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_2023 = json.load(open(data_dir / \"exist_2023\" / f\"{SPLIT}_t2_{LANG}.json\"))\n",
    "\n",
    "print(len(t2_2023))\n",
    "\n",
    "label2str = {\n",
    "    \"-\": \"4. not sexist\",\n",
    "    \"DIRECT\": \"1. direct\",\n",
    "    \"REPORTED\": \"2. reported\",\n",
    "    \"JUDGEMENTAL\": \"3. judgmental\",\n",
    "}\n",
    "\n",
    "\n",
    "def format_t2_2023(x):\n",
    "    label = Counter(x[\"value\"]).most_common(1)[0][0]\n",
    "\n",
    "    return {\"text\": prompt_t2_exist_2023 + x[\"tweet\"], \"response\": label2str[label]}\n",
    "\n",
    "def filter_t2_2023(x):\n",
    "    \"\"\"\n",
    "    Filter out samples that don't have agreement of at least half of the annotators.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = Counter(x[\"value\"])\n",
    "\n",
    "    for k, v in counts.items():\n",
    "        if v >=4:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "ds_t2_2023 = Dataset.from_list(t2_2023)\n",
    "ds_t2_2023 = ds_t2_2023.filter(filter_t2_2023).map(format_t2_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '300189',\n",
       " 'lang': 'es',\n",
       " 'tweet': 'Las universidades, preocupadas por los abusos sexuales entre los más jóvenesES LO QUE PASA CUANDO NO HAY, COMPROMISO Y EDUCACIÓN AFECTIVO SEXUAL DESDE MUY PEQUEÑOS Y EDUCACIÓN DEL BUEN TRATO https://t.co/EfcA2CKD6A',\n",
       " 'number_annotators': 6,\n",
       " 'annotators': ['Annotator_735',\n",
       "  'Annotator_736',\n",
       "  'Annotator_345',\n",
       "  'Annotator_737',\n",
       "  'Annotator_738',\n",
       "  'Annotator_148'],\n",
       " 'gender_annotators': ['F', 'F', 'F', 'M', 'M', 'M'],\n",
       " 'age_annotators': ['18-22', '23-45', '46+', '18-22', '23-45', '46+'],\n",
       " 'value': ['-', 'JUDGEMENTAL', '-', '-', 'JUDGEMENTAL', '-'],\n",
       " 'test_case': 'EXIST2023',\n",
       " 'text': '\"\"You are an excellent assistant at identifying the source in sexist spanish tweets. Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex. \\n\\nCategory definitions:\\n\\n1. Direct sexist message. The intention was to write a message that is sexist by itself or incites to be sexist, as in: \"Women shouldn\\'t code...perhaps be influencer/creator instead...it\\'s their natural\\nstrength.\"\\n2. Reported sexist message. The intention is to report and share a sexist situation suffered by a woman or women in first or third person, as in: \"Today, one of my year 1 class pupils could not believe he\\'d lost a race against a girl.\"\\n3. Judgemental message. The intention was judgmental, since the tweet describes sexist situations or behaviours with the aim of condemning them. As in: \"21st century and we are still earning 25% less than men #Idonotrenounce.\"\\n4. Not sexist message. No sexism present in the text.\\n\\nFor this task, you will be shown a tweet in Spanish. After thoroughly reading and analyzing the tweet, categorize the tweet into one of the above categories using the following labels:\\n\\n1. direct\\n2. reported\\n3. judgmental\\n4. not sexist\\n\\n\\nTweet:\\n\\nLas universidades, preocupadas por los abusos sexuales entre los más jóvenesES LO QUE PASA CUANDO NO HAY, COMPROMISO Y EDUCACIÓN AFECTIVO SEXUAL DESDE MUY PEQUEÑOS Y EDUCACIÓN DEL BUEN TRATO https://t.co/EfcA2CKD6A',\n",
       " 'response': '4. not sexist'}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_t2_2023.shuffle()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exist 2023 - Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an excellent assistant at categorizing sexism in spanish tweets. Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\n",
      "\n",
      "Here are the category definitions:\n",
      "\n",
      "1. Ideological and inequality. This category includes all tweets that discredit the feminist movement in order to devalue, belittle and defame the struggle of women in any aspect\n",
      "of their lives. It also includes messages that reject inequality between men and women, or present men as victims of gender-based oppression. Some examples of this type of sexism discourse can be found in the following tweets:  \"#Feminism is a war on men, but it's also a war on women. It's a war on female nature, a war on femininity.\"\n",
      "2. Role stereotyping and dominance. This category includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks. It also includes any claim that implies that men are somehow superior to women. \"Most women no longer have the desire or the knowledge to develop a high quality character, even if they wanted to.\"\n",
      "3. Objectification. Objectification and physical stereotyping includes messages where women are presented as objects apart from their dignity and personal aspects. We also include messages that assume or describe certain physical qualities that women must have in order to fulfill traditional gender roles, for example, ideas that suggest that women should maintain a standard and ideal of beauty or attacks on a woman's physique. \"No offense but I've never seen an attractive african american hooker. Not a single one\"\n",
      "4. Sexual violence. This category includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made: \"I wanna touch your tits..you can't imagine what I can do on your body\"\n",
      "5. Misogyny and non sexual violence: this category includes expressions of hatred and violence towards women: \"Domestic abuse is never okay.... Unless your wife is a bitch.\"\n",
      "6. Not sexist. No sexism present in the text.\n",
      "\n",
      "For this task, you will be shown a tweet in Spanish. After thoroughly reading and analyzing the tweet, categorize the tweet into ONE OR MORE of the above six categories using the following labels:\n",
      "\n",
      "1. ideological inequality\n",
      "2. stereotyping dominance\n",
      "3. sexual violence\n",
      "4. misogyny and/or non-sexual violence\n",
      "5. objectification\n",
      "6. not sexist\n",
      "\n",
      "\n",
      "Tweet:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_t3_exist_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16077550a71c4cf299e6336d99795812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e59e9da74e4ccda7f894051a488ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t3_2023 = json.load(open(data_dir / \"exist_2023\" / f\"{SPLIT}_t3_{LANG}.json\"))\n",
    "\n",
    "print(len(t3_2023))\n",
    "\n",
    "label2str = {'-': \"6. not sexist\", 'STEREOTYPING-DOMINANCE': \"2. stereotyping dominance\", 'OBJECTIFICATION': \"5. objectification\", 'IDEOLOGICAL-INEQUALITY': \"1. ideological inequality\", 'SEXUAL-VIOLENCE': \"3. sexual violence\", 'MISOGYNY-NON-SEXUAL-VIOLENCE': \"4. misogyny, non-sexual violence\"}\n",
    "\n",
    "def format_t3_2023(x):\n",
    "    labels = list(chain(*x[\"value\"]))\n",
    "\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    label_strs = []\n",
    "\n",
    "    for k, v in counts.items():\n",
    "        if v >=3:\n",
    "            label_strs.append(k)\n",
    "\n",
    "    return {\n",
    "        \"text\": prompt_t3_exist_2023 + x[\"tweet\"],\n",
    "        \"response\": \"\\n\".join(sorted(label2str[x] for x in label_strs))\n",
    "    }\n",
    "\n",
    "def filter_t3_2023(x):\n",
    "    \"\"\"\n",
    "    Filter out samples that don't have agreement between at least half of the annotators.\n",
    "    \"\"\"\n",
    "    labels = list(chain(*x[\"value\"]))\n",
    "\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    for k, v in counts.items():\n",
    "        if v >=3:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "ds_t3_2023 = Dataset.from_list(t3_2023)\n",
    "ds_t3_2023 = ds_t3_2023.filter(filter_t3_2023).map(format_t3_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '300153',\n",
       " 'lang': 'es',\n",
       " 'tweet': '@TDanganronper @IreneMontero La mujer promedio, aun sin identificarse como feminista, creerá en chaladuras como la opresión masculina o la cultura de la violación.',\n",
       " 'number_annotators': 6,\n",
       " 'annotators': ['Annotator_731',\n",
       "  'Annotator_732',\n",
       "  'Annotator_315',\n",
       "  'Annotator_6',\n",
       "  'Annotator_733',\n",
       "  'Annotator_734'],\n",
       " 'gender_annotators': ['F', 'F', 'F', 'M', 'M', 'M'],\n",
       " 'age_annotators': ['18-22', '23-45', '46+', '18-22', '23-45', '46+'],\n",
       " 'value': [['IDEOLOGICAL-INEQUALITY',\n",
       "   'STEREOTYPING-DOMINANCE',\n",
       "   'OBJECTIFICATION',\n",
       "   'SEXUAL-VIOLENCE',\n",
       "   'MISOGYNY-NON-SEXUAL-VIOLENCE'],\n",
       "  ['MISOGYNY-NON-SEXUAL-VIOLENCE'],\n",
       "  ['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE'],\n",
       "  ['IDEOLOGICAL-INEQUALITY',\n",
       "   'SEXUAL-VIOLENCE',\n",
       "   'MISOGYNY-NON-SEXUAL-VIOLENCE'],\n",
       "  ['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE'],\n",
       "  ['IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE']],\n",
       " 'test_case': 'EXIST2023',\n",
       " 'text': 'You are an excellent assistant at categorizing sexism in spanish tweets. Sexism is defined as prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.\\n\\nHere are the category definitions:\\n\\n1. Ideological and inequality. This category includes all tweets that discredit the feminist movement in order to devalue, belittle and defame the struggle of women in any aspect\\nof their lives. It also includes messages that reject inequality between men and women, or present men as victims of gender-based oppression. Some examples of this type of sexism discourse can be found in the following tweets:  \"#Feminism is a war on men, but it\\'s also a war on women. It\\'s a war on female nature, a war on femininity.\"\\n2. Role stereotyping and dominance. This category includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks. It also includes any claim that implies that men are somehow superior to women. \"Most women no longer have the desire or the knowledge to develop a high quality character, even if they wanted to.\"\\n3. Objectification. Objectification and physical stereotyping includes messages where women are presented as objects apart from their dignity and personal aspects. We also include messages that assume or describe certain physical qualities that women must have in order to fulfill traditional gender roles, for example, ideas that suggest that women should maintain a standard and ideal of beauty or attacks on a woman\\'s physique. \"No offense but I\\'ve never seen an attractive african american hooker. Not a single one\"\\n4. Sexual violence. This category includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made: \"I wanna touch your tits..you can\\'t imagine what I can do on your body\"\\n5. Misogyny and non sexual violence: this category includes expressions of hatred and violence towards women: \"Domestic abuse is never okay.... Unless your wife is a bitch.\"\\n6. Not sexist. No sexism present in the text.\\n\\nFor this task, you will be shown a tweet in Spanish. After thoroughly reading and analyzing the tweet, categorize the tweet into ONE OR MORE of the above six categories using the following labels:\\n\\n1. ideological inequality\\n2. stereotyping dominance\\n3. sexual violence\\n4. misogyny and/or non-sexual violence\\n5. objectification\\n6. not sexist\\n\\n\\nTweet:\\n\\n@TDanganronper @IreneMontero La mujer promedio, aun sin identificarse como feminista, creerá en chaladuras como la opresión masculina o la cultura de la violación.',\n",
       " 'response': '1. ideological inequality\\n2. stereotyping dominance\\n4. misogyny, non-sexual violence'}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_t3_2023.shuffle()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7f3690d8e141f5a290ecac08c2f7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc014a2733641dcb68d86401d8d16cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335380cdf4e04d2fbb056cd63da27787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa43cbd7fd34cc3964cb6517f70e607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8483bd1e174feca5b019d40070ef4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1331762"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cols = [\"text\", \"response\"]\n",
    "\n",
    "ds_t1_2022.remove_columns([x for x in ds_t1_2022.column_names if x not in final_cols]).to_parquet(data_dir / \"exist_2022\" / f\"{SPLIT}_t1_{LANG}_formatted.parquet\")\n",
    "ds_t2_2022.remove_columns([x for x in ds_t2_2022.column_names if x not in final_cols]).to_parquet(data_dir / \"exist_2022\" / f\"{SPLIT}_t2_{LANG}_formatted.parquet\")\n",
    "ds_t1_2023.remove_columns([x for x in ds_t1_2023.column_names if x not in final_cols]).to_parquet(data_dir / \"exist_2023\" / f\"{SPLIT}_t1_{LANG}_formatted.parquet\")\n",
    "ds_t2_2023.remove_columns([x for x in ds_t2_2023.column_names if x not in final_cols]).to_parquet(data_dir / \"exist_2023\" / f\"{SPLIT}_t2_{LANG}_formatted.parquet\")\n",
    "ds_t3_2023.remove_columns([x for x in ds_t3_2023.column_names if x not in final_cols]).to_parquet(data_dir / \"exist_2023\" / f\"{SPLIT}_t3_{LANG}_formatted.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
